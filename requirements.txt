--extra-index-url https://download.pytorch.org/whl/cu126
torch==2.6.0 
torchvision==0.21.0 
torchaudio==2.6.0
pip @ https://github.com/Dao-AILab/flash-attention/releases/download/v2.7.4.post1/flash_attn-2.7.4.post1+cu12torch2.6cxx11abiFALSE-cp310-cp310-linux_x86_64.whl
decord==0.6.0
einops==0.8.1
huggingface_hub==0.29.1
matplotlib==3.7.0
numpy==1.24.4
opencv_python==4.11.0.86
triton-windows==3.2.0.post19
pyarrow==11.0.0
PyYAML==6.0.2
Requests==2.32.3
safetensors==0.4.5
#scipy==1.10.1
scipy==1.15.3
sentencepiece==0.1.99
#torch==2.5.1
#torchvision==0.20.1
transformers==4.49.0
#flash_attn==2.5.8
#flash_attn
accelerate>=0.34.0
wandb
dfloat11
